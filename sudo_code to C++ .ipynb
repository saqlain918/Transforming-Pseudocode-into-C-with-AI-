{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 10081647,
          "sourceType": "datasetVersion",
          "datasetId": 6215376
        },
        {
          "sourceId": 10843670,
          "sourceType": "datasetVersion",
          "datasetId": 6734363
        }
      ],
      "dockerImageVersionId": 30805,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G0aSSJS68l5M",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-24T18:57:12.776326Z",
          "iopub.execute_input": "2025-02-24T18:57:12.777216Z",
          "iopub.status.idle": "2025-02-24T18:57:21.526033Z",
          "shell.execute_reply.started": "2025-02-24T18:57:12.777177Z",
          "shell.execute_reply": "2025-02-24T18:57:21.525040Z"
        }
      },
      "outputs": [],
      "execution_count": 59
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZTHWvRRhIfDi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-24T19:04:02.789300Z",
          "iopub.execute_input": "2025-02-24T19:04:02.789698Z",
          "iopub.status.idle": "2025-02-24T19:04:03.052230Z",
          "shell.execute_reply.started": "2025-02-24T19:04:02.789665Z",
          "shell.execute_reply": "2025-02-24T19:04:03.051177Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "tsv_file = \"samsum-train.csv.tsv\"\n",
        "csv_file = \"/content/samsum-train.csv\"\n",
        "df = pd.read_csv(tsv_file, sep='\\t')\n",
        "df = df.drop(columns=['workerid'])\n",
        "df = df.drop(columns=['probid'])\n",
        "df = df.drop(columns=['line'])\n",
        "df = df.drop(columns=['indent'])\n",
        "df = df.drop(columns=['subid'])\n",
        "df = df.rename(columns={'text': 'summary'})\n",
        "df = df.rename(columns={'code': 'dialogue'})\n",
        "df = df.head(10000)\n",
        "df.to_csv(csv_file, index=False)\n",
        "print(f\"TSV file '{tsv_file}' successfully converted to CSV as '{csv_file}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ2ajhl5Zt4d",
        "outputId": "c6c52c22-80e9-4ada-9edc-3a0ecc49d55c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TSV file 'samsum-train.csv.tsv' successfully converted to CSV as '/content/samsum-train.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('samsum-train.csv')\n",
        "df = df.dropna()\n",
        "df.to_csv('First_Row_New_Text_Document.csv', index=False)"
      ],
      "metadata": {
        "id": "GJekWQc5b_O9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-24T19:06:08.235235Z",
          "iopub.execute_input": "2025-02-24T19:06:08.236068Z",
          "iopub.status.idle": "2025-02-24T19:06:08.243574Z",
          "shell.execute_reply.started": "2025-02-24T19:06:08.236035Z",
          "shell.execute_reply": "2025-02-24T19:06:08.242347Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXrqXbLhZURG",
        "outputId": "ca304b63-688d-4ab4-8919-5ee6a0b20043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                summary  \\\n",
            "1                                       create string s   \n",
            "2                        create integers x1, y1, x2, y2   \n",
            "3                                                read s   \n",
            "4                                   set x1 to s[0] - 96   \n",
            "5                                  set y1 to s[1] - '0'   \n",
            "...                                                 ...   \n",
            "9994  if last is not equal to -1 set good[i + last +...   \n",
            "9995                                      set last to i   \n",
            "9997                for integer i = 0 to 1000 inclusive   \n",
            "9998        if ifprime(i) is false set good[i] to false   \n",
            "9999                            create integers n and k   \n",
            "\n",
            "                                        dialogue  \n",
            "1                                      string s;  \n",
            "2                            int x1, y1, x2, y2;  \n",
            "3                                      cin >> s;  \n",
            "4                                x1 = s[0] - 96;  \n",
            "5                               y1 = s[1] - '0';  \n",
            "...                                          ...  \n",
            "9994  if (last != -1) good[i + last + 1] = true;  \n",
            "9995                                   last = i;  \n",
            "9997             for (int i = 0; i <= 1000; i++)  \n",
            "9998           if (!ifprime(i)) good[i] = false;  \n",
            "9999                                   int n, k;  \n",
            "\n",
            "[7586 rows x 2 columns]\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-24T19:02:52.552193Z",
          "iopub.execute_input": "2025-02-24T19:02:52.552952Z",
          "iopub.status.idle": "2025-02-24T19:02:52.558305Z",
          "shell.execute_reply.started": "2025-02-24T19:02:52.552917Z",
          "shell.execute_reply": "2025-02-24T19:02:52.557327Z"
        },
        "id": "tBtvVxwmZURH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        " df = df.rename(columns={'dialogue': 'en', 'summary': 'it'})\n",
        "df.to_csv('First_Row_New_Text_Document.csv', index=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-24T19:06:04.329633Z",
          "iopub.execute_input": "2025-02-24T19:06:04.330308Z",
          "iopub.status.idle": "2025-02-24T19:06:04.338552Z",
          "shell.execute_reply.started": "2025-02-24T19:06:04.330272Z",
          "shell.execute_reply": "2025-02-24T19:06:04.337299Z"
        },
        "id": "NIW6m2iDZURI"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "t1_DJbxmdCSP",
        "outputId": "e5290b3f-fc5b-4f05-ed9d-11f24c46dd9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     it  \\\n",
              "1                                       create string s   \n",
              "2                        create integers x1, y1, x2, y2   \n",
              "3                                                read s   \n",
              "4                                   set x1 to s[0] - 96   \n",
              "5                                  set y1 to s[1] - '0'   \n",
              "...                                                 ...   \n",
              "9994  if last is not equal to -1 set good[i + last +...   \n",
              "9995                                      set last to i   \n",
              "9997                for integer i = 0 to 1000 inclusive   \n",
              "9998        if ifprime(i) is false set good[i] to false   \n",
              "9999                            create integers n and k   \n",
              "\n",
              "                                              en  \n",
              "1                                      string s;  \n",
              "2                            int x1, y1, x2, y2;  \n",
              "3                                      cin >> s;  \n",
              "4                                x1 = s[0] - 96;  \n",
              "5                               y1 = s[1] - '0';  \n",
              "...                                          ...  \n",
              "9994  if (last != -1) good[i + last + 1] = true;  \n",
              "9995                                   last = i;  \n",
              "9997             for (int i = 0; i <= 1000; i++)  \n",
              "9998           if (!ifprime(i)) good[i] = false;  \n",
              "9999                                   int n, k;  \n",
              "\n",
              "[7586 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9fa47b1-e0dc-449a-9d82-e45cbff1eda2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>it</th>\n",
              "      <th>en</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>create string s</td>\n",
              "      <td>string s;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>create integers x1, y1, x2, y2</td>\n",
              "      <td>int x1, y1, x2, y2;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>read s</td>\n",
              "      <td>cin &gt;&gt; s;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>set x1 to s[0] - 96</td>\n",
              "      <td>x1 = s[0] - 96;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>set y1 to s[1] - '0'</td>\n",
              "      <td>y1 = s[1] - '0';</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9994</th>\n",
              "      <td>if last is not equal to -1 set good[i + last +...</td>\n",
              "      <td>if (last != -1) good[i + last + 1] = true;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>set last to i</td>\n",
              "      <td>last = i;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>for integer i = 0 to 1000 inclusive</td>\n",
              "      <td>for (int i = 0; i &lt;= 1000; i++)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>if ifprime(i) is false set good[i] to false</td>\n",
              "      <td>if (!ifprime(i)) good[i] = false;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>create integers n and k</td>\n",
              "      <td>int n, k;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7586 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9fa47b1-e0dc-449a-9d82-e45cbff1eda2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f9fa47b1-e0dc-449a-9d82-e45cbff1eda2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f9fa47b1-e0dc-449a-9d82-e45cbff1eda2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ba81e47a-11ad-498c-8120-382f2b97ca22\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ba81e47a-11ad-498c-8120-382f2b97ca22')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ba81e47a-11ad-498c-8120-382f2b97ca22 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_0b53a7a9-435f-40a2-90d7-b263935aa2ed\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0b53a7a9-435f-40a2-90d7-b263935aa2ed button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 7586,\n  \"fields\": [\n    {\n      \"column\": \"it\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4166,\n        \"samples\": [\n          \"if a[i][j] = '*'\",\n          \"let a be an array of integers of size 10\",\n          \"increase s1[1]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"en\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2653,\n        \"samples\": [\n          \"bool o = true;\",\n          \"if (x == -1 && y == 1) return \\\"LU\\\";\",\n          \"int bao1 = 0;\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Cleaning"
      ],
      "metadata": {
        "id": "fCRBflQZ7vcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hBv9L9xCdBMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "df = pd.read_csv('First_Row_New_Text_Document.csv')\n",
        "#df = df.drop(columns=['id'])\n",
        "# Save the cleaned dataset to a new CSV file\n",
        "df = df.rename(columns={'dialogue': 'en', 'summary': 'it'})\n",
        "df.to_csv('First_Row_New_Text_Document.csv', index=False)\n",
        "print(df.iloc[:, -3:])\n",
        "print(\"The text has been cleaned and saved to 'cleaned.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5E2jh9k7u--",
        "outputId": "d7ca5199-b45f-4012-d4f4-0123565906e7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-24T19:03:49.590903Z",
          "iopub.execute_input": "2025-02-24T19:03:49.591614Z",
          "iopub.status.idle": "2025-02-24T19:03:50.024588Z",
          "shell.execute_reply.started": "2025-02-24T19:03:49.591570Z",
          "shell.execute_reply": "2025-02-24T19:03:50.023653Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id                                        translation\n",
            "0        0       {'en': 'string s;', 'it': 'create string s'}\n",
            "1        1  {'en': 'int x1, y1, x2, y2;', 'it': 'create in...\n",
            "2        2                {'en': 'cin >> s;', 'it': 'read s'}\n",
            "3        3  {'en': 'x1 = s[0] - 96;', 'it': 'set x1 to s[0...\n",
            "4        4  {'en': \"y1 = s[1] - '0';\", 'it': \"set y1 to s[...\n",
            "...    ...                                                ...\n",
            "7581  7581  {'en': 'if (last != -1) good[i + last + 1] = t...\n",
            "7582  7582         {'en': 'last = i;', 'it': 'set last to i'}\n",
            "7583  7583  {'en': 'for (int i = 0; i <= 1000; i++)', 'it'...\n",
            "7584  7584  {'en': 'if (!ifprime(i)) good[i] = false;', 'i...\n",
            "7585  7585  {'en': 'int n, k;', 'it': 'create integers n a...\n",
            "\n",
            "[7586 rows x 2 columns]\n",
            "The text has been cleaned and saved to 'cleaned.csv'\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Conversion\n"
      ],
      "metadata": {
        "id": "ohX8fkFR7qVt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KS2Gn8VB7e3D",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-24T19:05:44.899294Z",
          "iopub.execute_input": "2025-02-24T19:05:44.900158Z",
          "iopub.status.idle": "2025-02-24T19:05:45.053063Z",
          "shell.execute_reply.started": "2025-02-24T19:05:44.900124Z",
          "shell.execute_reply": "2025-02-24T19:05:45.051722Z"
        }
      },
      "outputs": [],
      "execution_count": 59
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "# Load the saved dataset\n",
        "dataset = load_from_disk(\"converted_conversation_dataset\")\n",
        "\n",
        "# Inspect the structure\n",
        "print(dataset)\n",
        "print(dataset[\"train\"][0])  # First record\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFL0x2-18i64",
        "outputId": "17fc6539-d296-44da-a5f8-778965bbf468",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:49.556299Z",
          "iopub.execute_input": "2024-12-04T13:58:49.556559Z",
          "iopub.status.idle": "2024-12-04T13:58:49.570698Z",
          "shell.execute_reply.started": "2024-12-04T13:58:49.556533Z",
          "shell.execute_reply": "2024-12-04T13:58:49.569836Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'translation'],\n",
            "        num_rows: 7586\n",
            "    })\n",
            "})\n",
            "{'id': '0', 'translation': {'en': 'string s;', 'it': 'create string s'}}\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeding Scaling and Forward\n"
      ],
      "metadata": {
        "id": "_j_Vb4eX88WZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_KNT9Y4wcqZe"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "VzA4FmiT84Mr",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:49.571904Z",
          "iopub.execute_input": "2024-12-04T13:58:49.572141Z",
          "iopub.status.idle": "2024-12-04T13:58:49.577592Z",
          "shell.execute_reply.started": "2024-12-04T13:58:49.572117Z",
          "shell.execute_reply": "2024-12-04T13:58:49.576995Z"
        }
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Embeding\n"
      ],
      "metadata": {
        "id": "7zKXqZPX96AZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Create a matrix of shape (seq_len, d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        # Create a vector of shape (seq_len)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
        "        # Create a vector of shape (d_model)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
        "        # Apply sine to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
        "        # Apply cosine to odd indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
        "        # Add a batch dimension to the positional encoding\n",
        "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
        "        # Register the positional encoding as a buffer\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "Rd2OlJ5e93oT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:49.578683Z",
          "iopub.execute_input": "2024-12-04T13:58:49.578991Z",
          "iopub.status.idle": "2024-12-04T13:58:49.589879Z",
          "shell.execute_reply.started": "2024-12-04T13:58:49.578966Z",
          "shell.execute_reply": "2024-12-04T13:58:49.589131Z"
        }
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Normalization"
      ],
      "metadata": {
        "id": "QExX2XBZCkhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
        "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, hidden_size)\n",
        "         # Keep the dimension for broadcasting\n",
        "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
        "        # Keep the dimension for broadcasting\n",
        "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
        "        # eps is to prevent dividing by zero or when std is very small\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
      ],
      "metadata": {
        "id": "qMBPl6bH_KmO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:49.592388Z",
          "iopub.execute_input": "2024-12-04T13:58:49.592845Z",
          "iopub.status.idle": "2024-12-04T13:58:49.602472Z",
          "shell.execute_reply.started": "2024-12-04T13:58:49.592818Z",
          "shell.execute_reply": "2024-12-04T13:58:49.601781Z"
        }
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed Forward"
      ],
      "metadata": {
        "id": "Pc_OEy4kC-K4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ],
      "metadata": {
        "id": "h2nQ-D0aCmTL",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:49.603574Z",
          "iopub.execute_input": "2024-12-04T13:58:49.603926Z",
          "iopub.status.idle": "2024-12-04T13:58:49.619197Z",
          "shell.execute_reply.started": "2024-12-04T13:58:49.603890Z",
          "shell.execute_reply": "2024-12-04T13:58:49.618602Z"
        }
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Head Attentation"
      ],
      "metadata": {
        "id": "rPKwhjnfEYJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # Embedding vector size\n",
        "        self.h = h # Number of heads\n",
        "        # Make sure d_model is divisible by h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        # Just apply the formula from the paper\n",
        "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
        "        # return attention scores which can be used for visualization\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Combine all the heads together\n",
        "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # Multiply by Wo\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)\n"
      ],
      "metadata": {
        "id": "l0bvwQkeDAmh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:50.288441Z",
          "iopub.execute_input": "2024-12-04T13:58:50.288771Z",
          "iopub.status.idle": "2024-12-04T13:58:50.298825Z",
          "shell.execute_reply.started": "2024-12-04T13:58:50.288742Z",
          "shell.execute_reply": "2024-12-04T13:58:50.297966Z"
        }
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Residual Connection"
      ],
      "metadata": {
        "id": "-qLEujmeEjpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "        def __init__(self, features: int, dropout: float) -> None:\n",
        "            super().__init__()\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            self.norm = LayerNormalization(features)\n",
        "\n",
        "        def forward(self, x, sublayer):\n",
        "            return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "0bw_97wZEbQt",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:50.401063Z",
          "iopub.execute_input": "2024-12-04T13:58:50.401311Z",
          "iopub.status.idle": "2024-12-04T13:58:50.406268Z",
          "shell.execute_reply.started": "2024-12-04T13:58:50.401287Z",
          "shell.execute_reply": "2024-12-04T13:58:50.405339Z"
        }
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "bWSK2s47E16D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x"
      ],
      "metadata": {
        "id": "IY07vD12EnDt",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:52.923444Z",
          "iopub.execute_input": "2024-12-04T13:58:52.924293Z",
          "iopub.status.idle": "2024-12-04T13:58:52.929866Z",
          "shell.execute_reply.started": "2024-12-04T13:58:52.924245Z",
          "shell.execute_reply": "2024-12-04T13:58:52.928991Z"
        }
      },
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "oZVIU88aE3WQ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:53.481688Z",
          "iopub.execute_input": "2024-12-04T13:58:53.482299Z",
          "iopub.status.idle": "2024-12-04T13:58:53.487334Z",
          "shell.execute_reply.started": "2024-12-04T13:58:53.482267Z",
          "shell.execute_reply": "2024-12-04T13:58:53.486434Z"
        }
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "cmTgDodvE8FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Hrx0w8DqE5jo",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:55.189127Z",
          "iopub.execute_input": "2024-12-04T13:58:55.189475Z",
          "iopub.status.idle": "2024-12-04T13:58:55.195946Z",
          "shell.execute_reply.started": "2024-12-04T13:58:55.189444Z",
          "shell.execute_reply": "2024-12-04T13:58:55.194809Z"
        }
      },
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "uRTBUM4hE-5i",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:55.857996Z",
          "iopub.execute_input": "2024-12-04T13:58:55.858655Z",
          "iopub.status.idle": "2024-12-04T13:58:55.863546Z",
          "shell.execute_reply.started": "2024-12-04T13:58:55.858605Z",
          "shell.execute_reply": "2024-12-04T13:58:55.862609Z"
        }
      },
      "outputs": [],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Projection Layer"
      ],
      "metadata": {
        "id": "kXOA9BWEFWvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x) -> None:\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return self.proj(x)"
      ],
      "metadata": {
        "id": "xjdS6GxdFAuJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:57.023819Z",
          "iopub.execute_input": "2024-12-04T13:58:57.024163Z",
          "iopub.status.idle": "2024-12-04T13:58:57.029123Z",
          "shell.execute_reply.started": "2024-12-04T13:58:57.024131Z",
          "shell.execute_reply": "2024-12-04T13:58:57.028147Z"
        }
      },
      "outputs": [],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "kGkGn-iUG742"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # (batch, seq_len, d_model)\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        # (batch, seq_len, d_model)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        # (batch, seq_len, vocab_size)\n",
        "        return self.projection_layer(x)"
      ],
      "metadata": {
        "id": "S68UHzopFaDY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:57.900799Z",
          "iopub.execute_input": "2024-12-04T13:58:57.901137Z",
          "iopub.status.idle": "2024-12-04T13:58:57.907511Z",
          "shell.execute_reply.started": "2024-12-04T13:58:57.901105Z",
          "shell.execute_reply": "2024-12-04T13:58:57.906657Z"
        }
      },
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
        "    # Create the embedding layers\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "hZuGcn1uG-0k",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:58.326764Z",
          "iopub.execute_input": "2024-12-04T13:58:58.327563Z",
          "iopub.status.idle": "2024-12-04T13:58:58.334906Z",
          "shell.execute_reply.started": "2024-12-04T13:58:58.327530Z",
          "shell.execute_reply": "2024-12-04T13:58:58.334050Z"
        }
      },
      "outputs": [],
      "execution_count": 28
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Greedy Decode"
      ],
      "metadata": {
        "id": "6GowcbHXIB_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset,DataLoader,random_split"
      ],
      "metadata": {
        "id": "EXjXRlnnHGcj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:59.212029Z",
          "iopub.execute_input": "2024-12-04T13:58:59.212634Z",
          "iopub.status.idle": "2024-12-04T13:58:59.218345Z",
          "shell.execute_reply.started": "2024-12-04T13:58:59.212581Z",
          "shell.execute_reply": "2024-12-04T13:58:59.217683Z"
        }
      },
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "    # Precompute the encoder output and reuse it for every step\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initialize the decoder input with the sos token\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        # build mask for target\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        # calculate output\n",
        "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "        # get next token\n",
        "        prob = model.project(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat(\n",
        "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
        "        )\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "zWrspGVAIPq6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:58:59.551597Z",
          "iopub.execute_input": "2024-12-04T13:58:59.551952Z",
          "iopub.status.idle": "2024-12-04T13:58:59.558690Z",
          "shell.execute_reply.started": "2024-12-04T13:58:59.551923Z",
          "shell.execute_reply": "2024-12-04T13:58:59.557677Z"
        }
      },
      "outputs": [],
      "execution_count": 30
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MvfGWaHaVck_",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:59:00.029692Z",
          "iopub.execute_input": "2024-12-04T13:59:00.030499Z",
          "iopub.status.idle": "2024-12-04T13:59:08.198800Z",
          "shell.execute_reply.started": "2024-12-04T13:59:00.030464Z",
          "shell.execute_reply": "2024-12-04T13:59:08.197576Z"
        }
      },
      "outputs": [],
      "execution_count": 59
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Validation"
      ],
      "metadata": {
        "id": "D6iol6joJLI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    source_texts = []\n",
        "    expected = []\n",
        "    predicted = []\n",
        "\n",
        "    try:\n",
        "        # get the console window width\n",
        "        with os.popen('stty size', 'r') as console:\n",
        "            _, console_width = console.read().split()\n",
        "            console_width = int(console_width)\n",
        "    except:\n",
        "        # If we can't get the console width, use 80 as default\n",
        "        console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
        "\n",
        "            # check that the batch size is 1\n",
        "            assert encoder_input.size(\n",
        "                0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"tgt_text\"][0]\n",
        "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "            source_texts.append(source_text)\n",
        "            expected.append(target_text)\n",
        "            predicted.append(model_out_text)\n",
        "\n",
        "            # Print the source, target and model output\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
        "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
        "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
        "\n",
        "            if count == num_examples:\n",
        "                print_msg('-'*console_width)\n",
        "                break\n",
        "\n",
        "    if writer:\n",
        "        # Evaluate the character error rate\n",
        "        # Compute the char error rate\n",
        "        metric = torchmetrics.CharErrorRate()\n",
        "        cer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation cer', cer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        # Compute the word error rate\n",
        "        metric = torchmetrics.WordErrorRate()\n",
        "        wer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation wer', wer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        # Compute the BLEU metric\n",
        "        metric = torchmetrics.BLEUScore()\n",
        "        bleu = metric(predicted, expected)\n",
        "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
        "        writer.flush()"
      ],
      "metadata": {
        "id": "XH6ykWv6IxWV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:59:08.201343Z",
          "iopub.execute_input": "2024-12-04T13:59:08.202146Z",
          "iopub.status.idle": "2024-12-04T13:59:09.610284Z",
          "shell.execute_reply.started": "2024-12-04T13:59:08.202100Z",
          "shell.execute_reply": "2024-12-04T13:59:09.609584Z"
        }
      },
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KYSMBdRI6XS-"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenzier"
      ],
      "metadata": {
        "id": "R4U8mkgOJQgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item['translation'][lang]"
      ],
      "metadata": {
        "id": "TRMmcNuSJbDQ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:59:09.611266Z",
          "iopub.execute_input": "2024-12-04T13:59:09.611798Z",
          "iopub.status.idle": "2024-12-04T13:59:09.615875Z",
          "shell.execute_reply.started": "2024-12-04T13:59:09.611770Z",
          "shell.execute_reply": "2024-12-04T13:59:09.615035Z"
        }
      },
      "outputs": [],
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "source": [
        "def get_or_build_tokenizer(config, ds, lang):\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "Z6QffvkLJNPQ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:59:09.618126Z",
          "iopub.execute_input": "2024-12-04T13:59:09.618428Z",
          "iopub.status.idle": "2024-12-04T13:59:09.629157Z",
          "shell.execute_reply.started": "2024-12-04T13:59:09.618384Z",
          "shell.execute_reply": "2024-12-04T13:59:09.628367Z"
        }
      },
      "outputs": [],
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "def get_ds(config):\n",
        "    # It only has the train split, so we divide it overselves\n",
        "    dataset_path = f\"{config['datasource']}\"\n",
        "    ds_raw = load_from_disk(dataset_path)\n",
        "    ds_raw = ds_raw['train']\n",
        "\n",
        "    # Build tokenizers\n",
        "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "    # Keep 90% for training, 10% for validation\n",
        "    train_ds_size = int(0.9 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) - train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    # Find the maximum length of each sentence in the source and target sentence\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
      ],
      "metadata": {
        "id": "-DRS2upmJQF2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:59:09.630065Z",
          "iopub.execute_input": "2024-12-04T13:59:09.630392Z",
          "iopub.status.idle": "2024-12-04T13:59:09.646490Z",
          "shell.execute_reply.started": "2024-12-04T13:59:09.630355Z",
          "shell.execute_reply": "2024-12-04T13:59:09.645806Z"
        }
      },
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Model"
      ],
      "metadata": {
        "id": "Srh-H9vaQMk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "dPa9j97TQMA3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:59:09.647459Z",
          "iopub.execute_input": "2024-12-04T13:59:09.647735Z",
          "iopub.status.idle": "2024-12-04T13:59:09.663212Z",
          "shell.execute_reply.started": "2024-12-04T13:59:09.647711Z",
          "shell.execute_reply": "2024-12-04T13:59:09.662561Z"
        }
      },
      "outputs": [],
      "execution_count": 37
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "q9DdZ4I7QfQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "def train_model(config):\n",
        "    # Define the device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "    if (device == 'cuda'):\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "    elif (device == 'mps'):\n",
        "        print(f\"Device name: <mps>\")\n",
        "    else:\n",
        "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
        "        print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
        "        print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
        "    device = torch.device(device)\n",
        "\n",
        "    # Make sure the weights folder exists\n",
        "    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "    # Tensorboard\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "    # If the user specified a model to preload before training, load it\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "    preload = config['preload']\n",
        "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
        "    if model_filename:\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "    else:\n",
        "        print('No model to preload, starting from scratch')\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
        "\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "        if epoch==1 or epoch==0:\n",
        "            print(\"nothing to remove\")\n",
        "        elif epoch <= 11:\n",
        "            epoch_num=epoch-2\n",
        "            os.remove(f\"converted_conversation_dataset_weights/tmodel_0{epoch_num}.pt\")\n",
        "        else:\n",
        "            epoch_num=epoch-2\n",
        "            os.remove(f\"/converted_conversation_dataset_weights/tmodel_{epoch_num}.pt\")\n",
        "        torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "        for batch in batch_iterator:\n",
        "\n",
        "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
        "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
        "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
        "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
        "\n",
        "            # Run the tensors through the encoder, decoder and the projection layer\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
        "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
        "\n",
        "            # Compare the output with the label\n",
        "            label = batch['label'].to(device) # (B, seq_len)\n",
        "\n",
        "            # Compute the loss using a simple cross entropy\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            # Log the loss\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        # Run validation at the end of every epoch\n",
        "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "        # Save the model at the end of every epoch\n",
        "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step\n",
        "        }, model_filename)\n"
      ],
      "metadata": {
        "id": "EHupc8sUJhqi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:59:09.664173Z",
          "iopub.execute_input": "2024-12-04T13:59:09.664391Z",
          "iopub.status.idle": "2024-12-04T13:59:09.679372Z",
          "shell.execute_reply.started": "2024-12-04T13:59:09.664361Z",
          "shell.execute_reply": "2024-12-04T13:59:09.678679Z"
        }
      },
      "outputs": [],
      "execution_count": 57
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the model"
      ],
      "metadata": {
        "id": "Rh5kdy8bRByQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "if __name__ == '__main__':\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    config = get_config()\n",
        "    train_model(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9BIEUdYQhIU",
        "outputId": "2e1ad7d0-2738-4bf9-ab8a-e1d708cf132f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T14:00:02.765883Z",
          "iopub.execute_input": "2024-12-04T14:00:02.766522Z",
          "iopub.status.idle": "2024-12-04T17:03:20.241154Z",
          "shell.execute_reply.started": "2024-12-04T14:00:02.766475Z",
          "shell.execute_reply": "2024-12-04T17:03:20.239448Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Device name: Tesla T4\n",
            "Device memory: 14.74127197265625 GB\n",
            "Max length of source sentence: 47\n",
            "Max length of target sentence: 53\n",
            "Preloading model converted_conversation_dataset_weights/tmodel_01.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 02: 100%|██████████| 854/854 [07:37<00:00,  1.87it/s, loss=2.120]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: string s, t;\n",
            "    TARGET: let s and t be strings\n",
            " PREDICTED: create strings s , t\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: cout << \".\";\n",
            "    TARGET: print \".\"\n",
            " PREDICTED: print \".\"\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "execution_count": 58
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Scores"
      ],
      "metadata": {
        "id": "zHVE37TbZURY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import altair as alt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T17:06:55.514762Z",
          "iopub.execute_input": "2024-12-04T17:06:55.515339Z",
          "iopub.status.idle": "2024-12-04T17:06:55.751465Z",
          "shell.execute_reply.started": "2024-12-04T17:06:55.515308Z",
          "shell.execute_reply": "2024-12-04T17:06:55.750533Z"
        },
        "id": "VFeqLupaZURY"
      },
      "outputs": [],
      "execution_count": 39
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T17:07:12.182796Z",
          "iopub.execute_input": "2024-12-04T17:07:12.183437Z",
          "iopub.status.idle": "2024-12-04T17:07:12.187777Z",
          "shell.execute_reply.started": "2024-12-04T17:07:12.183401Z",
          "shell.execute_reply": "2024-12-04T17:07:12.186825Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEA0hUjyZURY",
        "outputId": "a4dc8702-79fe-4b61-ad96-93566e2e4215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "execution_count": 40
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T17:08:02.033127Z",
          "iopub.execute_input": "2024-12-04T17:08:02.033927Z",
          "iopub.status.idle": "2024-12-04T17:08:06.384761Z",
          "shell.execute_reply.started": "2024-12-04T17:08:02.033891Z",
          "shell.execute_reply": "2024-12-04T17:08:06.383870Z"
        },
        "id": "NKhSuQShZURZ"
      },
      "outputs": [],
      "execution_count": 59
    },
    {
      "cell_type": "code",
      "source": [
        "def load_next_batch():\n",
        "    # Load a sample batch from the validation set\n",
        "    batch = next(iter(val_dataloader))\n",
        "    encoder_input = batch[\"encoder_input\"].to(device)\n",
        "    encoder_mask = batch[\"encoder_mask\"].to(device)\n",
        "    decoder_input = batch[\"decoder_input\"].to(device)\n",
        "    decoder_mask = batch[\"decoder_mask\"].to(device)\n",
        "\n",
        "    encoder_input_tokens = [vocab_src.id_to_token(idx) for idx in encoder_input[0].cpu().numpy()]\n",
        "    decoder_input_tokens = [vocab_tgt.id_to_token(idx) for idx in decoder_input[0].cpu().numpy()]\n",
        "\n",
        "    # check that the batch size is 1\n",
        "    assert encoder_input.size(\n",
        "        0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "    model_out = greedy_decode(\n",
        "        model, encoder_input, encoder_mask, vocab_src, vocab_tgt, config['seq_len'], device)\n",
        "\n",
        "    return batch, encoder_input_tokens, decoder_input_tokens"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T17:08:16.477735Z",
          "iopub.execute_input": "2024-12-04T17:08:16.478066Z",
          "iopub.status.idle": "2024-12-04T17:08:16.483951Z",
          "shell.execute_reply.started": "2024-12-04T17:08:16.478039Z",
          "shell.execute_reply": "2024-12-04T17:08:16.483097Z"
        },
        "id": "dgydqJhcZURZ"
      },
      "outputs": [],
      "execution_count": 45
    },
    {
      "cell_type": "code",
      "source": [
        "def mtx2df(m, max_row, max_col, row_tokens, col_tokens):\n",
        "    return pd.DataFrame(\n",
        "        [\n",
        "            (\n",
        "                r,\n",
        "                c,\n",
        "                float(m[r, c]),\n",
        "                \"%.3d %s\" % (r, row_tokens[r] if len(row_tokens) > r else \"<blank>\"),\n",
        "                \"%.3d %s\" % (c, col_tokens[c] if len(col_tokens) > c else \"<blank>\"),\n",
        "            )\n",
        "            for r in range(m.shape[0])\n",
        "            for c in range(m.shape[1])\n",
        "            if r < max_row and c < max_col\n",
        "        ],\n",
        "        columns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
        "    )\n",
        "\n",
        "def get_attn_map(attn_type: str, layer: int, head: int):\n",
        "    if attn_type == \"encoder\":\n",
        "        attn = model.encoder.layers[layer].self_attention_block.attention_scores\n",
        "    elif attn_type == \"decoder\":\n",
        "        attn = model.decoder.layers[layer].self_attention_block.attention_scores\n",
        "    elif attn_type == \"encoder-decoder\":\n",
        "        attn = model.decoder.layers[layer].cross_attention_block.attention_scores\n",
        "    return attn[0, head].data\n",
        "\n",
        "def attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len):\n",
        "    df = mtx2df(\n",
        "        get_attn_map(attn_type, layer, head),\n",
        "        max_sentence_len,\n",
        "        max_sentence_len,\n",
        "        row_tokens,\n",
        "        col_tokens,\n",
        "    )\n",
        "    return (\n",
        "        alt.Chart(data=df)\n",
        "        .mark_rect()\n",
        "        .encode(\n",
        "            x=alt.X(\"col_token\", axis=alt.Axis(title=\"\")),\n",
        "            y=alt.Y(\"row_token\", axis=alt.Axis(title=\"\")),\n",
        "            color=\"value\",\n",
        "            tooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
        "        )\n",
        "        #.title(f\"Layer {layer} Head {head}\")\n",
        "        .properties(height=400, width=400, title=f\"Layer {layer} Head {head}\")\n",
        "        .interactive()\n",
        "    )\n",
        "\n",
        "def get_all_attention_maps(attn_type: str, layers: list[int], heads: list[int], row_tokens: list, col_tokens, max_sentence_len: int):\n",
        "    charts = []\n",
        "    for layer in layers:\n",
        "        rowCharts = []\n",
        "        for head in heads:\n",
        "            rowCharts.append(attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len))\n",
        "        charts.append(alt.hconcat(*rowCharts))\n",
        "    return alt.vconcat(*charts)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T17:08:39.615902Z",
          "iopub.execute_input": "2024-12-04T17:08:39.616688Z",
          "iopub.status.idle": "2024-12-04T17:08:39.625946Z",
          "shell.execute_reply.started": "2024-12-04T17:08:39.616652Z",
          "shell.execute_reply": "2024-12-04T17:08:39.625047Z"
        },
        "id": "skD9Y4cvZURZ"
      },
      "outputs": [],
      "execution_count": 46
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T17:08:51.487601Z",
          "iopub.execute_input": "2024-12-04T17:08:51.488486Z",
          "iopub.status.idle": "2024-12-04T17:08:51.752143Z",
          "shell.execute_reply.started": "2024-12-04T17:08:51.488450Z",
          "shell.execute_reply": "2024-12-04T17:08:51.751236Z"
        },
        "id": "hLAa7SU1ZURZ"
      },
      "outputs": [],
      "execution_count": 59
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T17:09:38.556924Z",
          "iopub.execute_input": "2024-12-04T17:09:38.557529Z",
          "iopub.status.idle": "2024-12-04T17:09:40.088724Z",
          "shell.execute_reply.started": "2024-12-04T17:09:38.557493Z",
          "shell.execute_reply": "2024-12-04T17:09:40.087690Z"
        },
        "id": "9593-ExJZURa"
      },
      "outputs": [],
      "execution_count": 59
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove the previous trained model"
      ],
      "metadata": {
        "id": "b4NlkaKtZURa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data set configration"
      ],
      "metadata": {
        "id": "4YRQiGlhReOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "\n",
        "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_target_pair = self.ds[idx]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        # Transform the text into tokens\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        # Add sos, eos and padding to each sentence\n",
        "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
        "        # We will only add <s>, and </s> only on the label\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "        # Add <s> and </s> token\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only <s> token\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only </s> token\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Double check the size of the tensors to make sure they are all seq_len long\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input,  # (seq_len)\n",
        "            \"decoder_input\": decoder_input,  # (seq_len)\n",
        "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
        "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
        "            \"label\": label,  # (seq_len)\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text,\n",
        "        }"
      ],
      "metadata": {
        "id": "KMzeet8nRP98",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:59:32.321881Z",
          "iopub.execute_input": "2024-12-04T13:59:32.322599Z",
          "iopub.status.idle": "2024-12-04T13:59:32.334762Z",
          "shell.execute_reply.started": "2024-12-04T13:59:32.322562Z",
          "shell.execute_reply": "2024-12-04T13:59:32.333681Z"
        }
      },
      "outputs": [],
      "execution_count": 49
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "    return mask == 0"
      ],
      "metadata": {
        "id": "JrBpz2b9RQlu",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:59:37.710229Z",
          "iopub.execute_input": "2024-12-04T13:59:37.710899Z",
          "iopub.status.idle": "2024-12-04T13:59:37.715014Z",
          "shell.execute_reply.started": "2024-12-04T13:59:37.710865Z",
          "shell.execute_reply": "2024-12-04T13:59:37.714001Z"
        }
      },
      "outputs": [],
      "execution_count": 50
    },
    {
      "cell_type": "code",
      "source": [
        "t = Summery(\"int i = 15;\")\n",
        "print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvk2tDeTam-q",
        "outputId": "59e035cc-a3e5-489b-a295-9fe1c3b7dc68",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T17:05:41.608815Z",
          "iopub.execute_input": "2024-12-04T17:05:41.609403Z",
          "iopub.status.idle": "2024-12-04T17:05:43.357436Z",
          "shell.execute_reply.started": "2024-12-04T17:05:41.609368Z",
          "shell.execute_reply": "2024-12-04T17:05:43.356511Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "    SOURCE: int i = 15;\n",
            " PREDICTED: create integer i with value   create integer i with value\n"
          ]
        }
      ],
      "execution_count": 59
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "DmaNujVCRpwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_config():\n",
        "    return {\n",
        "        \"batch_size\": 8,\n",
        "        \"num_epochs\": 3,\n",
        "        \"lr\": 10**-4,\n",
        "        \"seq_len\": 500,\n",
        "        \"d_model\": 512,\n",
        "        \"datasource\": 'converted_conversation_dataset',  # Local path\n",
        "        \"lang_src\": \"en\",\n",
        "        \"lang_tgt\": \"it\",\n",
        "        \"model_folder\": \"weights\",\n",
        "        \"model_basename\": \"tmodel_\",\n",
        "        \"preload\": \"latest\",\n",
        "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
        "        \"experiment_name\": \"runs/tmodel\"\n",
        "    }"
      ],
      "metadata": {
        "id": "UpH6cEOeRTnv",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:59:48.137887Z",
          "iopub.execute_input": "2024-12-04T13:59:48.138232Z",
          "iopub.status.idle": "2024-12-04T13:59:48.143288Z",
          "shell.execute_reply.started": "2024-12-04T13:59:48.138200Z",
          "shell.execute_reply": "2024-12-04T13:59:48.142309Z"
        }
      },
      "outputs": [],
      "execution_count": 53
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
        "    return str(Path('.') / model_folder / model_filename)\n",
        "\n",
        "# Find the latest weights file in the weights folder\n",
        "def latest_weights_file_path(config):\n",
        "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}*\"\n",
        "    weights_files = list(Path(model_folder).glob(model_filename))\n",
        "    if len(weights_files) == 0:\n",
        "        return None\n",
        "    weights_files.sort()\n",
        "    return str(weights_files[-1])"
      ],
      "metadata": {
        "id": "Qnjhog_LRuLq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:59:48.801488Z",
          "iopub.execute_input": "2024-12-04T13:59:48.802283Z",
          "iopub.status.idle": "2024-12-04T13:59:48.807605Z",
          "shell.execute_reply.started": "2024-12-04T13:59:48.802237Z",
          "shell.execute_reply": "2024-12-04T13:59:48.806600Z"
        }
      },
      "outputs": [],
      "execution_count": 54
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import Tokenizer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import sys\n",
        "from datasets import load_from_disk\n",
        "def Summery(sentence: str):\n",
        "    # Define the device, tokenizers, and model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "    config = get_config()\n",
        "    tokenizer_src = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_src']))))\n",
        "    tokenizer_tgt = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_tgt']))))\n",
        "    model = build_transformer(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size(), config[\"seq_len\"], config['seq_len'], d_model=config['d_model']).to(device)\n",
        "\n",
        "    # Load the pretrained weights\n",
        "    model_filename = latest_weights_file_path(config)\n",
        "    state = torch.load(model_filename)\n",
        "    model.load_state_dict(state['model_state_dict'])\n",
        "\n",
        "    # if the sentence is a number use it as an index to the test set\n",
        "    label = \"\"\n",
        "    if type(sentence) == int or sentence.isdigit():\n",
        "        id = int(sentence)\n",
        "        dataset_path = f\"{config['datasource']}\"\n",
        "        ds_raw = load_from_disk(dataset_path)\n",
        "        ds = ds_raw['train']\n",
        "        ds = BilingualDataset(ds, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "        sentence = ds[id]['src_text']\n",
        "        label = ds[id][\"tgt_text\"]\n",
        "    seq_len = config['seq_len']\n",
        "\n",
        "    # translate the sentence\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Precompute the encoder output and reuse it for every generation step\n",
        "        source = tokenizer_src.encode(sentence)\n",
        "        source = torch.cat([\n",
        "            torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64),\n",
        "            torch.tensor(source.ids, dtype=torch.int64),\n",
        "            torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64),\n",
        "            torch.tensor([tokenizer_src.token_to_id('[PAD]')] * (seq_len - len(source.ids) - 2), dtype=torch.int64)\n",
        "        ], dim=0).to(device)\n",
        "        source_mask = (source != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(device)\n",
        "        encoder_output = model.encode(source, source_mask)\n",
        "\n",
        "        # Initialize the decoder input with the sos token\n",
        "        decoder_input = torch.empty(1, 1).fill_(tokenizer_tgt.token_to_id('[SOS]')).type_as(source).to(device)\n",
        "\n",
        "        # Print the source sentence and target start prompt\n",
        "        if label != \"\": print(f\"{f'ID: ':>12}{id}\")\n",
        "        print(f\"{f'SOURCE: ':>12}{sentence}\")\n",
        "        if label != \"\": print(f\"{f'TARGET: ':>12}{label}\")\n",
        "        print(f\"{f'PREDICTED: ':>12}\", end='')\n",
        "\n",
        "        # Generate the translation word by word\n",
        "        while decoder_input.size(1) < seq_len:\n",
        "            # build mask for target and calculate output\n",
        "            decoder_mask = torch.triu(torch.ones((1, decoder_input.size(1), decoder_input.size(1))), diagonal=1).type(torch.int).type_as(source_mask).to(device)\n",
        "            out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "            # project next token\n",
        "            prob = model.project(out[:, -1])\n",
        "            _, next_word = torch.max(prob, dim=1)\n",
        "            decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "            # print the translated word\n",
        "            print(f\"{tokenizer_tgt.decode([next_word.item()])}\", end=' ')\n",
        "\n",
        "            # break if we predict the end of sentence token\n",
        "            if next_word == tokenizer_tgt.token_to_id('[EOS]'):\n",
        "                break\n",
        "\n",
        "    # convert ids to tokens\n",
        "    return tokenizer_tgt.decode(decoder_input[0].tolist())\n",
        "\n",
        "#read sentence from argument\n",
        "#translate(sys.argv[1] if len(sys.argv) > 1 else \"I am not a very good a student.\")"
      ],
      "metadata": {
        "id": "_amfH91WR1bM",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:59:49.397076Z",
          "iopub.execute_input": "2024-12-04T13:59:49.397426Z",
          "iopub.status.idle": "2024-12-04T13:59:49.410845Z",
          "shell.execute_reply.started": "2024-12-04T13:59:49.397394Z",
          "shell.execute_reply": "2024-12-04T13:59:49.409833Z"
        }
      },
      "outputs": [],
      "execution_count": 55
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install translation_utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjO6b_NrJ7ne",
        "outputId": "77c75a5b-38e5-4fbe-e6d4-6f82b92c7fc9"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement translation_utils (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for translation_utils\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from tokenizers import Tokenizer\n",
        "from pathlib import Path\n",
        "\n",
        "def Summery(cpp_code: str):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    config = get_config()\n",
        "\n",
        "    tokenizer_src = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_src']))))\n",
        "    tokenizer_tgt = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_tgt']))))\n",
        "\n",
        "    model = build_transformer(\n",
        "        tokenizer_src.get_vocab_size(),\n",
        "        tokenizer_tgt.get_vocab_size(),\n",
        "        config[\"seq_len\"],\n",
        "        config['seq_len'],\n",
        "        d_model=config['d_model']\n",
        "    ).to(device)\n",
        "\n",
        "    model_filename = latest_weights_file_path(config)\n",
        "    state = torch.load(model_filename, map_location=device)\n",
        "    model.load_state_dict(state['model_state_dict'])\n",
        "\n",
        "    model.eval()\n",
        "    seq_len = config['seq_len']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        source = tokenizer_src.encode(cpp_code)\n",
        "        source = torch.cat([\n",
        "            torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64),\n",
        "            torch.tensor(source.ids, dtype=torch.int64),\n",
        "            torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64),\n",
        "            torch.tensor([tokenizer_src.token_to_id('[PAD]')] * (seq_len - len(source.ids) - 2), dtype=torch.int64)\n",
        "        ], dim=0).to(device)\n",
        "\n",
        "        source_mask = (source != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(device)\n",
        "        encoder_output = model.encode(source, source_mask)\n",
        "\n",
        "        decoder_input = torch.empty(1, 1).fill_(tokenizer_tgt.token_to_id('[SOS]')).type_as(source).to(device)\n",
        "\n",
        "        output_words = []\n",
        "        while decoder_input.size(1) < seq_len:\n",
        "            decoder_mask = torch.triu(torch.ones((1, decoder_input.size(1), decoder_input.size(1))), diagonal=1).type(torch.int).type_as(source_mask).to(device)\n",
        "            out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "            prob = model.project(out[:, -1])\n",
        "            _, next_word = torch.max(prob, dim=1)\n",
        "            decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "            word = tokenizer_tgt.decode([next_word.item()])\n",
        "            output_words.append(word)\n",
        "\n",
        "            if next_word == tokenizer_tgt.token_to_id('[EOS]'):\n",
        "                break\n",
        "\n",
        "    return \" \".join(output_words)\n",
        "\n",
        "def get_config():\n",
        "    return {\n",
        "        \"batch_size\": 8,\n",
        "        \"num_epochs\": 3,\n",
        "        \"lr\": 10**-4,\n",
        "        \"seq_len\": 500,\n",
        "        \"d_model\": 512,\n",
        "        \"datasource\": 'converted_conversation_dataset',\n",
        "        \"lang_src\": \"en\",\n",
        "        \"lang_tgt\": \"it\",\n",
        "        \"model_folder\": \"weights\",\n",
        "        \"model_basename\": \"tmodel_\",\n",
        "        \"preload\": \"latest\",\n",
        "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
        "        \"experiment_name\": \"runs/tmodel\"\n",
        "    }\n",
        "\n",
        "def latest_weights_file_path(config):\n",
        "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}*\"\n",
        "    weights_files = list(Path(model_folder).glob(model_filename))\n",
        "    if len(weights_files) == 0:\n",
        "        return None\n",
        "    weights_files.sort()\n",
        "    return str(weights_files[-1])\n",
        "\n",
        "def gradio_interface(cpp_input):\n",
        "    if not cpp_input.strip():\n",
        "        return \"Please enter some C++ code to convert\"\n",
        "    try:\n",
        "        result = Summery(cpp_input)\n",
        "        result = result.replace('[SOS]', '').replace('[EOS]', '').replace('[PAD]', '').strip()\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=gr.Textbox(\n",
        "        lines=10,\n",
        "        placeholder=\"Enter your C++ code here...\",\n",
        "        label=\"C++ Code Input\"\n",
        "    ),\n",
        "    outputs=gr.Textbox(\n",
        "        label=\"Generated Pseudocode\"\n",
        "    ),\n",
        "    title=\"C++ to Pseudocode Transformer\",\n",
        "    description=\"Use AI to convert C++ code into structured pseudocode. Paste your C++ code below and get an AI-generated pseudocode version instantly.\",\n",
        "    theme=\"compact\",\n",
        "    allow_flagging=\"never\",\n",
        "    css=\"\"\"\n",
        "        .gradio-container {\n",
        "            font-family: 'Poppins', sans-serif;\n",
        "            background: linear-gradient(135deg, #1E29B, #33415);\n",
        "            color: white;\n",
        "            text-align: center;\n",
        "            padding: 30px;\n",
        "        }\n",
        "        textarea {\n",
        "            background: #0f172a !important;\n",
        "            color: white !important;\n",
        "            font-size: 16px !important;\n",
        "            border-radius: 10px !important;\n",
        "            padding: 15px !important;\n",
        "        }\n",
        "        .gr-button {\n",
        "            background: #3B82F6 !important;\n",
        "            color: white !important;\n",
        "            font-weight: bold !important;\n",
        "            padding: 10px 20px !important;\n",
        "            border-radius: 8px !important;\n",
        "        }\n",
        "        .gr-button:hover {\n",
        "            background: #2563EB !important;\n",
        "        }\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "-8xb0bEKMKcA",
        "outputId": "008df606-fa3e-4ada-9f85-6f65d06c56e6"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3d9009f03f3eb25639.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3d9009f03f3eb25639.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "la8qwjfePPtg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}